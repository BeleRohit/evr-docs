{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EVERSANA ETL Process Runbook: Your Guide to Data at EVERSANA","text":"<p>Version: 1.0 Date: July 17, 2025  </p>"},{"location":"#purpose","title":"Purpose","text":"<p>Welcome to the team! \ud83d\udc4b</p> <p>We're excited to have you on board! At EVERSANA, data is at the heart of what we do. Whether it\u2019s improving healthcare outcomes, analyzing market trends, or helping our clients make smarter decisions\u2014our data systems power the insights behind it all.</p> <p>This runbook is designed to give you a clear, step-by-step introduction to our data pipeline, even if you're completely new to these systems. Think of it as a map to our data landscape\u2014you'll learn how raw information comes in, how it\u2019s cleaned and organized, and how it becomes ready for analysis and business use.</p>"},{"location":"#1-welcome-to-the-team","title":"1. Welcome to the Team! \ud83d\udc4b","text":"<p>At EVERSANA, we think of data operations like a factory assembly line:</p> <ul> <li>We bring in raw data from multiple sources (clients, partners, internal systems).</li> <li>We clean, standardize, and transform that data to make it useful.</li> <li>We store it in secure, scalable systems where our analysts and applications can easily access it.</li> </ul> <p>This document will guide you through each part of that process so you can hit the ground running.</p>"},{"location":"#2-quick-glossary-understanding-our-lingo","title":"2. Quick Glossary: Understanding Our Lingo \ud83d\udcd6","text":"<p>Here are some key terms you'll encounter in your work. Don\u2019t worry about memorizing them all\u2014this section is here for quick reference whenever you need it!</p>"},{"location":"#etl","title":"ETL","text":"<p>Extract, Transform, Load</p> <p>This is our core data pipeline process:</p> <ul> <li>Extract: We pull data from various sources. This could be client systems, APIs, files in storage, or databases.</li> <li>Transform: We clean the data, reshape it into the correct formats, remove duplicates, handle missing values, and sometimes combine it with other data sources.</li> <li>Load: We store the transformed data into our central data warehouse (Snowflake) so it\u2019s ready for reporting, analytics, or downstream systems.</li> </ul> <p>ETL is like cooking: you gather ingredients (extract), prepare and cook them (transform), and serve the final dish (load).</p>"},{"location":"#elt","title":"ELT","text":"<p>Extract, Load, Transform</p> <p>This is a variation of ETL where:</p> <ul> <li>We Extract data from the source.</li> <li>We Load the raw data directly into Snowflake (our data warehouse).</li> <li>We then Transform the data inside Snowflake using SQL.</li> </ul> <p>This approach is useful when you want to retain raw data for auditing or historical reference while transforming only the parts you need later.</p>"},{"location":"#dag","title":"DAG","text":"<p>Directed Acyclic Graph</p> <p>In Apache Airflow, a DAG is like a flowchart that defines the sequence of tasks in a pipeline.</p> <ul> <li>Each task represents a step in a data process (like extracting data, running a validation, or moving files).</li> <li>The \"Directed\" part means the tasks have a specific order.</li> <li>The \"Acyclic\" part means there are no loops\u2014tasks don\u2019t go in circles.</li> </ul> <p>Think of a DAG as the blueprint that Airflow follows to automate our ETL processes.</p>"},{"location":"#aws","title":"AWS","text":"<p>Amazon Web Services</p> <p>AWS is our cloud computing platform. We use it to host almost all of our data systems. It provides computing power, storage, databases, networking, and security services\u2014all accessible over the internet.</p>"},{"location":"#aws-s3","title":"AWS S3","text":"<p>Simple Storage Service</p> <p>S3 is Amazon\u2019s cloud storage service. Imagine it as having access to infinite folders in the cloud where we can store any kind of file\u2014raw data, logs, reports, and backups.</p> <ul> <li>It\u2019s scalable: handles petabytes of data.</li> <li>It\u2019s secure: with encryption, access controls, and auditing.</li> <li>We often use S3 as the first landing zone for incoming client files.</li> </ul>"},{"location":"#apache-airflow","title":"Apache Airflow","text":"<p>Workflow Orchestration Tool</p> <p>Airflow is our scheduler and orchestrator for data pipelines.</p> <ul> <li>It decides what task runs when, in what order, and under what conditions.</li> <li>Airflow handles retries if something fails, sends alerts, and keeps track of execution logs.</li> </ul> <p>Think of it as the conductor of the data orchestra, making sure each system plays its part at the right time.</p>"},{"location":"#aws-ecs","title":"AWS ECS","text":"<p>Elastic Container Service</p> <p>ECS lets us run Docker containers (packaged applications) in the cloud.</p> <ul> <li>It provides container orchestration\u2014deciding where and how containers run.</li> <li>It manages resource scaling so we can handle large workloads without worrying about underlying servers.</li> </ul> <p>We use ECS to run parts of our ETL pipelines, custom APIs, and processing jobs.</p>"},{"location":"#aws-fargate","title":"AWS Fargate","text":"<p>Serverless Containers</p> <p>AWS Fargate is a special mode of running containers in ECS without managing servers.</p> <ul> <li>We tell Fargate how much CPU and memory we need.</li> <li>AWS handles all the infrastructure behind the scenes.</li> </ul> <p>This allows us to focus only on building data applications, not on server setup or maintenance.</p>"},{"location":"#docker","title":"Docker","text":"<p>Containerization Technology</p> <p>Docker allows us to package applications into self-contained units called containers.  </p> <ul> <li>Each container includes the application code, dependencies, libraries, and configurations.</li> <li>This ensures the code runs the same way everywhere\u2014whether on your laptop, in AWS, or in ECS.</li> </ul> <p>Docker simplifies deployment, testing, and scaling.</p>"},{"location":"#snowflake","title":"Snowflake","text":"<p>Cloud Data Warehouse</p> <p>Snowflake is where we store, process, and query most of our data.</p> <ul> <li>It\u2019s highly scalable and handles large datasets easily.</li> <li>We use it for analytics, reporting, dashboards, and data sharing.</li> </ul> <p>Snowflake supports SQL-based querying, making it accessible to both technical and non-technical users.</p>"},{"location":"#cicd","title":"CI/CD","text":"<p>Continuous Integration / Continuous Deployment</p> <p>This is our automated pipeline for code changes:</p> <ul> <li>Continuous Integration (CI): We automatically build and test code whenever changes are made. This helps catch bugs early.</li> <li>Continuous Deployment (CD): After testing, the code is automatically deployed to production or staging environments without manual steps.</li> </ul> <p>CI/CD allows us to release updates quickly and reliably, improving both speed and quality.</p>"},{"location":"#dqm","title":"DQM","text":"<p>Data Quality Monitoring</p> <p>DQM ensures our data is:</p> <ul> <li>Accurate: Correct and reliable.</li> <li>Complete: No missing critical information.</li> <li>Consistent: Matches expected formats and rules.</li> </ul> <p>We have systems in place to validate data at every step, from ingestion to storage.</p>"},{"location":"#metadata","title":"Metadata","text":"<p>Data About Data</p> <p>Metadata describes:</p> <ul> <li>What the data is (e.g., file names, column descriptions).</li> <li>Where it came from (source systems, extraction dates).</li> <li>How it should be processed (transformation rules, data types).</li> </ul> <p>We store metadata in special tables, making it easier to automate ETL jobs and maintain data lineage.</p>"},{"location":"#end-of-document","title":"End of Document","text":"<p>Welcome again! If you ever get stuck or have questions, don\u2019t hesitate to reach out to the team. We\u2019re here to help you succeed.</p>"},{"location":"ci_cd/","title":"4. How We Build &amp; Deploy Code: Our CI/CD Process \ud83d\ude80","text":"<p>Our CI/CD (Continuous Integration / Continuous Deployment) process is the automated pipeline we use to move new code from a developer\u2019s laptop into our live data systems safely and efficiently.</p> <p></p> <p>We use Azure DevOps for:</p> <ul> <li>Version control (code repositories via Azure Repos)  </li> <li>CI/CD pipelines (automated testing and deployment)  </li> <li>Code review and project management</li> </ul>"},{"location":"ci_cd/#overview-of-the-cicd-pipeline","title":"Overview of the CI/CD Pipeline","text":""},{"location":"ci_cd/#1-you-write-code","title":"1\ufe0f\u20e3 You Write Code","text":"<p>As a developer, you'll:</p> <ul> <li>Write new code for our data processes, such as:  </li> <li>Apache Airflow DAGs (for scheduling data workflows)  </li> <li>Data transformation scripts (ETL/ELT code)  </li> <li>Work inside a feature branch of our code repository (Azure Repos).</li> </ul> <p>This ensures your changes are isolated and easy to review.</p>"},{"location":"ci_cd/#2-propose-changes-create-a-pull-request-pr","title":"2\ufe0f\u20e3 Propose Changes (Create a Pull Request - PR)","text":"<p>Once your feature is ready:</p> <ul> <li>You create a Pull Request (PR) in Azure DevOps.  </li> <li>A PR is a formal request to merge your feature branch into the <code>dev</code> branch.</li> </ul> <p>This step kicks off the review and testing process.</p> <p>Learn how to create a Pull Request in Azure DevOps</p>"},{"location":"ci_cd/#3-automated-checks-continuous-integration-ci","title":"3\ufe0f\u20e3 Automated Checks (Continuous Integration - CI)","text":"<p>When you create a PR, our CI pipeline automatically runs:</p>"},{"location":"ci_cd/#code-quality-checks","title":"\ud83d\udd0d Code Quality Checks","text":"<ul> <li>We use SonarCloud Analysis to:</li> <li>Scan for bugs  </li> <li>Identify security issues  </li> <li>Detect code smells (bad practices)</li> </ul>"},{"location":"ci_cd/#build-components","title":"\ud83d\udee0\ufe0f Build Components","text":"<ul> <li>The pipeline builds necessary components, such as:</li> <li>Docker images for containerized data jobs  </li> <li>Packaging Airflow DAGs or Python scripts for deployment  </li> </ul>"},{"location":"ci_cd/#pass-requirements","title":"\u2705 Pass Requirements","text":"<p>Your PR can only be merged when:</p> <ul> <li>All automated checks pass </li> <li>At least one team member reviews and approves your code</li> </ul> <p>Code reviews are essential for:</p> <ul> <li>Catching bugs early  </li> <li>Sharing knowledge  </li> <li>Maintaining code quality and consistency</li> </ul>"},{"location":"ci_cd/#4-merge-to-dev","title":"4\ufe0f\u20e3 Merge to <code>dev</code>","text":"<p>Once approved, your changes are merged into the <code>dev</code> branch.</p> <p>This is our development environment where new code is first tested end-to-end.</p>"},{"location":"ci_cd/#5-deploy-to-development-continuous-deployment-cd","title":"5\ufe0f\u20e3 Deploy to Development (Continuous Deployment - CD)","text":"<p>Merging into <code>dev</code> triggers the CD pipeline:</p> <ul> <li>Your code is automatically deployed to our development data pipelines.  </li> <li>This lets the team test new workflows in a safe environment before going live.</li> </ul>"},{"location":"ci_cd/#6-deploy-to-production","title":"6\ufe0f\u20e3 Deploy to Production","text":"<p>After successful testing in development:</p> <ul> <li>Your team creates a PR from <code>dev</code> to <code>main</code> (the production branch).</li> <li>Once approved, the production CD pipeline deploys your code to live systems.</li> </ul>"},{"location":"ci_cd/#why-cicd","title":"Why CI/CD?","text":"<p>CI/CD is crucial for modern data engineering because it:</p> <ul> <li>Speeds up deployment \u2013 Faster delivery of new features and bug fixes.  </li> <li>Reduces human error \u2013 Automated pipelines catch issues early.  </li> <li>Ensures consistency \u2013 Same process for every deployment.  </li> <li>Improves collaboration \u2013 Code reviews promote shared knowledge and quality control.</li> </ul>"},{"location":"ci_cd/#code-review-best-practices","title":"Code Review Best Practices","text":"<p>When reviewing or submitting a PR:</p> <ul> <li>Write clear, descriptive PR titles and messages.</li> <li>Keep PRs small and focused (one change per PR if possible).  </li> <li>Use comments to explain non-obvious code decisions.  </li> <li>Be constructive in feedback\u2014focus on improvement, not criticism.  </li> <li>Review logic, security, and performance, not just syntax.</li> </ul>"},{"location":"ci_cd/#tools-involved","title":"Tools Involved","text":"Tool Purpose Azure DevOps Version control, PRs, pipelines Azure Repos Git-based code repository SonarCloud Code quality and security checks Docker Packaging and running code in containers Airflow Scheduling and orchestrating data workflows"},{"location":"ci_cd/#end-of-cicd-section","title":"End of CI/CD Section","text":""},{"location":"deployement_guide/","title":"EVERSANA Code Deployment Guide: How to Deploy Your Code \ud83d\ude80","text":"<p>Version: 1.0 Date: July 17, 2025  </p>"},{"location":"deployement_guide/#purpose","title":"Purpose","text":"<p>Welcome to the EVERSANA developer workflow!</p> <p>As a developer, you\u2019ll regularly need to deploy your code changes into our systems. This document provides a complete, step-by-step guide to help you understand how code deployment works at EVERSANA, even if you're new to Git or Azure DevOps.</p> <p>By the end of this guide, you\u2019ll know how to:</p> <ul> <li>Work with Git branches  </li> <li>Push your code to Azure Repos  </li> <li>Create Pull Requests (PRs)  </li> <li>Deploy to development and production environments  </li> <li>Verify your deployment</li> </ul>"},{"location":"deployement_guide/#deployment-workflow-overview","title":"Deployment Workflow Overview","text":"<p>We use a branch-based development workflow combined with CI/CD pipelines for safe and efficient code deployment.</p>"},{"location":"deployement_guide/#branching-model","title":"Branching Model:","text":"Branch Purpose <code>dev</code> Active development and testing environment <code>main</code> Production-ready, stable code <code>feature/*</code> New features or bug fixes (temporary branch)"},{"location":"deployement_guide/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Start by cloning the code repository from Azure Repos to your local machine.</p>"},{"location":"deployement_guide/#command","title":"Command:","text":"<pre><code>git clone &lt;repository-url&gt;\n````\n\nReplace `&lt;repository-url&gt;` with the link shared by your team.\n\nThis will download the repository to your machine.\n\n---\n\n## Step 2: Update Your Local `dev` Branch\n\nMake sure your local `dev` branch is **up-to-date** before starting any new work.\n\n### Commands:\n\n```bash\ngit checkout dev          # Switch to the dev branch\ngit pull origin dev       # Update local dev with the latest changes\n</code></pre> <p>This ensures you are working on the latest codebase.</p>"},{"location":"deployement_guide/#step-3-create-a-new-feature-branch","title":"Step 3: Create a New Feature Branch","text":"<p>All new work should be done in a separate feature branch to keep changes isolated and reviewable.</p>"},{"location":"deployement_guide/#command_1","title":"Command:","text":"<pre><code>git checkout -b your-descriptive-branch-name\n</code></pre>"},{"location":"deployement_guide/#naming-convention-example","title":"Naming Convention Example:","text":"Type Example Feature <code>feature/add-new-dag</code> Bugfix <code>bugfix/fix-s3-path</code> Hotfix <code>hotfix/urgent-dag-fix</code>"},{"location":"deployement_guide/#step-4-make-your-changes","title":"Step 4: Make Your Changes","text":"<p>Edit the relevant files depending on your task.</p> <p>Examples:</p> <ul> <li>Apache Airflow DAGs \u2013 Define or update task workflows.</li> <li>Python scripts \u2013 Implement transformations or business logic.</li> <li>SQL scripts \u2013 Modify data queries or models.</li> <li>Configuration files \u2013 Update environment settings or parameters.</li> </ul>"},{"location":"deployement_guide/#step-5-stage-and-commit-your-changes","title":"Step 5: Stage and Commit Your Changes","text":"<p>Once your changes are ready:</p>"},{"location":"deployement_guide/#stage-the-changes","title":"Stage the changes:","text":"<pre><code>git add .\n</code></pre>"},{"location":"deployement_guide/#commit-the-changes-with-a-clear-message","title":"Commit the changes with a clear message:","text":"<pre><code>git commit -m \"Short, clear description of your changes\"\n</code></pre>"},{"location":"deployement_guide/#useful-git-commands","title":"Useful Git Commands","text":"Action Command Stage all changes <code>git add .</code> Commit changes <code>git commit -m \"Your message\"</code> Check status <code>git status</code> View unstaged changes <code>git diff</code> View commit history <code>git log</code>"},{"location":"deployement_guide/#step-6-push-your-feature-branch","title":"Step 6: Push Your Feature Branch","text":"<p>Push your new branch to Azure Repos so that others can review your code.</p>"},{"location":"deployement_guide/#command_2","title":"Command:","text":"<pre><code>git push -u origin your-descriptive-branch-name\n</code></pre> <p>The <code>-u</code> flag links your local branch to the remote branch for easier future pushes.</p>"},{"location":"deployement_guide/#step-7-create-a-pull-request-pr-to-dev","title":"Step 7: Create a Pull Request (PR) to <code>dev</code>","text":"<p>In Azure DevOps, create a Pull Request (PR):</p> <ol> <li>Navigate to your repository in Azure DevOps.</li> <li>Open a PR from your feature branch into <code>dev</code>.</li> <li>Provide a clear title and description of what your PR does.</li> <li>Assign reviewers (your team will guide you on reviewers).</li> <li>Azure Pipelines will run automated checks (tests, linting, etc.).</li> </ol>"},{"location":"deployement_guide/#step-8-merge-into-dev","title":"Step 8: Merge into <code>dev</code>","text":"<p>Once your PR is:</p> <ul> <li>Approved by reviewers</li> <li>All checks pass</li> </ul> <p>You can merge your changes into the <code>dev</code> branch.</p>"},{"location":"deployement_guide/#what-happens-next","title":"What happens next?","text":"<ul> <li>Your code is automatically deployed to the development environment via CI/CD pipelines.</li> <li>This allows your team to test the changes in a safe environment.</li> </ul>"},{"location":"deployement_guide/#step-9-create-a-pull-request-pr-to-main","title":"Step 9: Create a Pull Request (PR) to <code>main</code>","text":"<p>After successful testing in the development environment:</p> <ol> <li>Create a new PR from <code>dev</code> to <code>main</code>.</li> <li>This PR is reviewed again to ensure production readiness.</li> <li>Automated checks will run as part of the pipeline.</li> </ol>"},{"location":"deployement_guide/#step-10-merge-into-main-production-deployment","title":"Step 10: Merge into <code>main</code> (Production Deployment)","text":"<p>When the <code>main</code> PR is:</p> <ul> <li>Approved by reviewers</li> <li>All checks pass</li> </ul> <p>Merging the PR will automatically deploy your changes to production.</p> <p>Our CI/CD pipeline handles the deployment process\u2014no manual server steps are needed.</p>"},{"location":"deployement_guide/#step-11-verify-your-deployment","title":"Step 11: Verify Your Deployment","text":"<p>After production deployment:</p> <ol> <li> <p>Check the Airflow UI to confirm:</p> </li> <li> <p>New DAGs appear (if applicable).</p> </li> <li> <p>DAGs are scheduled and running properly.</p> </li> <li> <p>Monitor Logs for any errors or unexpected behavior.</p> </li> <li> <p>Test Data Pipelines if your changes affect ETL jobs.</p> </li> </ol>"},{"location":"deployement_guide/#troubleshooting-tips","title":"Troubleshooting Tips","text":""},{"location":"deployement_guide/#common-deployment-issues","title":"Common Deployment Issues:","text":"Issue Where to Check Pipeline failure Azure DevOps Pipeline logs Merge conflicts Local Git terminal DAG not visible in Airflow Airflow UI logs &amp; DAG folder structure Failing automated tests Azure Pipeline test output Environment variable/config errors Pipeline configs &amp; deployment logs"},{"location":"deployement_guide/#quick-git-command-reference","title":"Quick Git Command Reference","text":"Action Command Clone repository <code>git clone &lt;repository-url&gt;</code> Switch to a branch <code>git checkout &lt;branch-name&gt;</code> Create new branch <code>git checkout -b &lt;new-branch-name&gt;</code> Pull latest changes <code>git pull origin &lt;branch-name&gt;</code> Stage all changes <code>git add .</code> Commit changes <code>git commit -m \"Your message\"</code> Check status <code>git status</code> View unstaged changes <code>git diff</code> View commit history <code>git log</code> Push new branch <code>git push -u origin &lt;branch-name&gt;</code>"},{"location":"deployement_guide/#summary-deployment-flow","title":"Summary: Deployment Flow","text":"<pre><code>Local Feature Branch  \n        \u2502\n        \u2514\u2500\u2500&gt; Pull Request to `dev`  \n                \u2502\n                \u2514\u2500\u2500&gt; Automated Tests &amp; Review  \n                        \u2502\n                        \u2514\u2500\u2500&gt; Merge to `dev`  \n                                \u2502\n                                \u2514\u2500\u2500&gt; Deploy to Development  \n                                        \u2502\n                                        \u2514\u2500\u2500&gt; Pull Request to `main`  \n                                                \u2502\n                                                \u2514\u2500\u2500&gt; Review &amp; Approval  \n                                                        \u2502\n                                                        \u2514\u2500\u2500&gt; Merge to `main`  \n                                                                \u2502\n                                                                \u2514\u2500\u2500&gt; Deploy to Production\n</code></pre>"},{"location":"deployement_guide/#final-notes","title":"Final Notes","text":"<ul> <li>Always verify your deployments in Airflow and monitor logs post-deployment.</li> <li>Use clear commit messages and descriptive branch names to make the process collaborative and easy to review.</li> <li>If you encounter issues, ask your team leads or check Azure DevOps logs for details.</li> </ul>"},{"location":"glossary/","title":"Full Glossary of Terms","text":"<p>This page provides a comprehensive list of terms used throughout the EVERSANA runbook and within our data team.</p>"},{"location":"glossary/#etl-extract-transform-load","title":"ETL (Extract, Transform, Load)","text":"<p>Our main data processing pipeline:</p> <ul> <li>Extract data from its source.</li> <li>Transform it (clean, reshape, combine).</li> <li>Load it into its final destination (usually Snowflake).</li> </ul> <p>Learn more about ETL</p>"},{"location":"glossary/#elt-extract-load-transform","title":"ELT (Extract, Load, Transform)","text":"<p>A variation of ETL where:</p> <ul> <li>Data is Extracted from the source.</li> <li>Loaded into the target system (like Snowflake).</li> <li>Then Transformed inside that system using SQL or other tools.</li> </ul> <p>We use both ETL and ELT depending on the use case.</p> <p>Learn more about ELT</p>"},{"location":"glossary/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"<p>In Apache Airflow, a DAG represents:</p> <ul> <li>A flowchart of tasks that define a specific data process.</li> <li>Tasks run in sequence or parallel based on dependencies.</li> <li>Acyclic means tasks move forward without loops.</li> </ul> <p>Learn more about Airflow DAGs</p>"},{"location":"glossary/#aws-amazon-web-services","title":"AWS (Amazon Web Services)","text":"<p>The cloud platform where EVERSANA hosts:</p> <ul> <li>Data storage  </li> <li>Compute workloads  </li> <li>Cloud services for scalability and reliability</li> </ul> <p>Learn more about AWS</p>"},{"location":"glossary/#aws-s3-simple-storage-service","title":"AWS S3 (Simple Storage Service)","text":"<p>Amazon\u2019s object storage service, used for:</p> <ul> <li>Storing raw data files  </li> <li>Keeping backups  </li> <li>Managing large-scale data securely in the cloud</li> </ul> <p>Learn more about AWS S3</p>"},{"location":"glossary/#apache-airflow","title":"Apache Airflow","text":"<p>An open-source workflow orchestrator:</p> <ul> <li>Manages scheduling, monitoring, and execution of data pipelines.</li> <li>Ensures tasks run in the correct sequence with error handling.</li> </ul> <p>Visit the Apache Airflow documentation</p>"},{"location":"glossary/#aws-ecs-elastic-container-service","title":"AWS ECS (Elastic Container Service)","text":"<p>AWS ECS is used to run containerized applications:</p> <ul> <li>Supports Docker containers in the cloud.</li> <li>Manages deployment and scaling automatically.</li> </ul> <p>Learn more about AWS ECS</p>"},{"location":"glossary/#aws-fargate","title":"AWS Fargate","text":"<p>A serverless compute engine for containers:</p> <ul> <li>Works with ECS to run containers without managing infrastructure.</li> <li>Automatically provisions and scales resources.</li> </ul> <p>Learn more about AWS Fargate</p>"},{"location":"glossary/#docker","title":"Docker","text":"<p>A containerization platform:</p> <ul> <li>Packages applications and dependencies into self-contained units called containers.</li> <li>Ensures software runs the same way in any environment.</li> </ul> <p>Visit the Docker documentation</p>"},{"location":"glossary/#snowflake","title":"Snowflake","text":"<p>Our cloud-based data warehouse:</p> <ul> <li>Stores processed data.</li> <li>Supports advanced analytics and reporting via SQL.</li> <li>Highly scalable and supports multiple workloads.</li> </ul> <p>Visit the Snowflake documentation</p>"},{"location":"glossary/#cicd-continuous-integration-continuous-deployment","title":"CI/CD (Continuous Integration / Continuous Deployment)","text":"<p>A system for automated building, testing, and deploying of code:</p> <ul> <li>CI: Automatically tests and integrates code changes.  </li> <li>CD: Automatically deploys code to development and production environments.</li> </ul> <p>Learn more about CI/CD</p>"},{"location":"glossary/#dqm-data-quality-monitoring","title":"DQM (Data Quality Monitoring)","text":"<p>A process for checking the quality of our data:</p> <ul> <li>Validates data accuracy, completeness, and consistency.</li> <li>Prevents bad data from entering downstream systems.</li> </ul> <p>Learn more about Data Quality</p>"},{"location":"glossary/#metadata","title":"Metadata","text":"<p>\"Data about data.\"</p> <p>In EVERSANA's systems, metadata includes:</p> <ul> <li>Column descriptions  </li> <li>File formats  </li> <li>Processing rules  </li> <li>Data lineage and audit information</li> </ul> <p>Learn more about Metadata</p>"},{"location":"glossary/#azure-devops","title":"Azure DevOps","text":"<p>A suite of tools from Microsoft for:</p> <ul> <li>Version control (Git)  </li> <li>CI/CD pipeline management  </li> <li>Project tracking and collaboration</li> </ul> <p>Visit the Azure DevOps documentation</p>"},{"location":"glossary/#azure-repos","title":"Azure Repos","text":"<p>The Git-based code repository service within Azure DevOps:</p> <ul> <li>Stores and manages our source code.  </li> <li>Supports pull requests, branching, and code reviews.</li> </ul> <p>Learn more about Azure Repos</p>"},{"location":"glossary/#tableau","title":"Tableau","text":"<p>A business intelligence (BI) tool used for:</p> <ul> <li>Data visualization  </li> <li>Interactive dashboards  </li> <li>Reporting and analytics for business users</li> </ul> <p>Visit the Tableau documentation</p>"},{"location":"glossary/#sonarcloud-analysis","title":"SonarCloud Analysis","text":"<p>A cloud-based code quality and security analysis tool:</p> <ul> <li>Performs static code analysis during CI/CD pipelines.  </li> <li>Identifies bugs, security vulnerabilities, and code smells.</li> </ul> <p>Visit the SonarCloud documentation</p>"},{"location":"metadata_tables/","title":"6. Our Metadata Tables: The Pipeline's Brains \ud83e\udde0","text":"<p>At EVERSANA, our data pipeline is smart and flexible because it's driven by metadata.</p>"},{"location":"metadata_tables/#what-is-metadata","title":"What is Metadata?","text":"<p>Metadata means \"data about data.\" In our pipelines, metadata tells the system:</p> <ul> <li>What files to load </li> <li>Where to find them </li> <li>What columns they contain </li> <li>What transformations to apply </li> <li>How to handle errors or exceptions</li> </ul> <p>This removes the need to hardcode rules into scripts. Instead, we define rules once in metadata tables, and the pipeline reads them dynamically at runtime.</p> <p>All metadata is stored in Snowflake, our cloud data warehouse.</p>"},{"location":"metadata_tables/#why-use-metadata","title":"Why Use Metadata?","text":"Benefit Description Flexibility New files and columns can be added without rewriting code. Reusability Same pipeline logic works for multiple datasets. Transparency Clear documentation of how data is processed. Error Reduction Fewer manual code changes = fewer bugs. Simplified Troubleshooting Metadata tracks file status and process flows."},{"location":"metadata_tables/#key-metadata-tables-in-our-pipeline","title":"Key Metadata Tables in Our Pipeline","text":"<p>Below are the most important metadata tables you'll encounter:</p>"},{"location":"metadata_tables/#1-dynamic_data_loads","title":"1\ufe0f\u20e3 DYNAMIC_DATA_LOADS","text":""},{"location":"metadata_tables/#purpose","title":"Purpose:","text":"<p>Stores instructions for how raw data is loaded into the system.</p>"},{"location":"metadata_tables/#what-it-defines","title":"What it Defines:","text":"Column Description <code>FILE ID</code> Unique identifier for each input file <code>S3 FILE PATH</code> AWS S3 location of the file <code>FREQUENCY</code> How often we expect the file (e.g., Daily, Weekly) <code>RAW LOAD_TYPE</code> Loading method (e.g., <code>FIRST_TIME</code>, <code>TRUNCATE</code>)"},{"location":"metadata_tables/#example-query","title":"Example Query:","text":"<pre><code>SELECT \"FILE ID\", \"S3 FILE PATH\", \"FREQUENCY\", \"RAW LOAD_TYPE\"\nFROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_DATA_LOADS\nLIMIT 5;\n````\n\n---\n\n### **2\ufe0f\u20e3 DYNAMIC\\_COLUMN\\_DEFINITIONS**\n\n#### **Purpose:**\n\nActs as a **data dictionary** for every table and file column in our pipeline.\n\n#### **What it Defines:**\n\n| Column                        | Description                                   |\n| ----------------------------- | --------------------------------------------- |\n| `TABLE IDENTIFIER`            | The name of the table or dataset              |\n| `COLUMN NAME`                 | The name of each column                       |\n| `DATATYPE`                    | The data type (`TEXT`, `FIXED`, `DATE`, etc.) |\n| `COLUMN_VALUE_TRANSFORMATION` | Custom rules for transforming column values   |\n\n#### **Example Query:**\n\n```sql\nSELECT \"TABLE IDENTIFIER\", \"COLUMN NAME\", \"DATATYPE\", \"COLUMN_VALUE_TRANSFORMATION\"\nFROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_COLUMN_DEFINITIONS\nWHERE \"TABLE IDENTIFIER\" = 'YOUR_EXAMPLE_TABLE'\nLIMIT 5;\n</code></pre>"},{"location":"metadata_tables/#3-dynamic_core_loads","title":"3\ufe0f\u20e3 DYNAMIC_CORE_LOADS","text":""},{"location":"metadata_tables/#purpose_1","title":"Purpose:","text":"<p>Defines how data is processed in the Core Layer, where cleaning, validation, and transformation happen.</p>"},{"location":"metadata_tables/#what-it-defines_1","title":"What it Defines:","text":"Column Description <code>TABLE IDENTIFIER</code> The target table name <code>CORE LOAD TYPE</code> Load type for core processing (<code>DELTA</code>, <code>UPSERT</code>) <code>ENRICHMENT FLAG</code> Indicates if additional enrichment is required"},{"location":"metadata_tables/#example-query_1","title":"Example Query:","text":"<pre><code>SELECT \"TABLE IDENTIFIER\", \"CORE LOAD TYPE\", \"ENRICHMENT FLAG\"\nFROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_CORE_LOADS\nLIMIT 5;\n</code></pre>"},{"location":"metadata_tables/#4-dynamic_ftb_input_universe","title":"4\ufe0f\u20e3 DYNAMIC_FTB_INPUT_UNIVERSE","text":""},{"location":"metadata_tables/#purpose_2","title":"Purpose:","text":"<p>Tracks the status of incoming files as they move through pipeline stages.</p>"},{"location":"metadata_tables/#what-it-defines_2","title":"What it Defines:","text":"Column Description <code>FILE ID</code> Unique identifier for the file <code>FILE NAME</code> Actual file name in S3 <code>PRE VALIDATION</code> Status of pre-processing validation checks <code>CORE LOAD</code> Status of core data load <code>ERROR</code> Any error messages encountered <code>SYSTEM DATE</code> Timestamp of the latest update"},{"location":"metadata_tables/#example-query_2","title":"Example Query:","text":"<pre><code>SELECT \"FILE ID\", \"FILE NAME\", \"PRE VALIDATION\", \"CORE LOAD\", \"ERROR\"\nFROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_FTB_INPUT_UNIVERSE\nORDER BY \"SYSTEM DATE\" DESC\nLIMIT 5;\n</code></pre>"},{"location":"metadata_tables/#5-dynamic_enrichment_loads","title":"5\ufe0f\u20e3 DYNAMIC_ENRICHMENT_LOADS","text":""},{"location":"metadata_tables/#purpose_3","title":"Purpose:","text":"<p>Stores custom SQL queries for data enrichment tasks.</p> <p>Enrichment means adding additional value to data, like:</p> <ul> <li>Looking up reference data</li> <li>Adding customer details from master tables</li> <li>Generating calculated fields</li> </ul>"},{"location":"metadata_tables/#what-it-defines_3","title":"What it Defines:","text":"Column Description <code>TABLE IDENTIFIER</code> The target table for enrichment <code>SQL_QUERY</code> The custom SQL to run for enrichment"},{"location":"metadata_tables/#example-query_3","title":"Example Query:","text":"<pre><code>SELECT \"TABLE_IDENTIFIER\", \"SQL_QUERY\"\nFROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_ENRICHMENT_LOADS\nWHERE \"TABLE_IDENTIFIER\" = 'YOUR_ENRICHMENT_TABLE'\nLIMIT 1;\n</code></pre>"},{"location":"metadata_tables/#how-these-tables-work-together","title":"How These Tables Work Together","text":"Table Name Connects To Relationship Example <code>DYNAMIC_DATA_LOADS</code> <code>DYNAMIC_FTB_INPUT_UNIVERSE</code> Both share <code>FILE ID</code> for file tracking <code>DYNAMIC_COLUMN_DEFINITIONS</code> All load processes Defines column structure for loading <code>DYNAMIC_CORE_LOADS</code> <code>DYNAMIC_DATA_LOADS</code> Controls next steps after raw load <code>DYNAMIC_ENRICHMENT_LOADS</code> <code>DYNAMIC_CORE_LOADS</code> Executes after core load if enrichment is flagged"},{"location":"metadata_tables/#summary","title":"Summary","text":"<p>By using metadata-driven pipelines, EVERSANA ensures:</p> <ul> <li>Agility: We can onboard new data sources with minimal code changes.</li> <li>Standardization: All data loads follow consistent, well-documented rules.</li> <li>Transparency: Anyone can inspect the metadata tables to understand pipeline behavior.</li> <li>Troubleshooting Power: You can trace any file or table through the pipeline stages.</li> </ul>"},{"location":"security_monitoring/","title":"5. Staying Secure &amp; Monitoring Everything \ud83d\udd12\ud83d\udc40","text":"<p>At EVERSANA, security and monitoring are built into every step of our data pipeline.  </p> <p>We prioritize:</p> <ul> <li>Data protection </li> <li>System reliability </li> <li>Proactive alerts and monitoring </li> </ul>"},{"location":"security_monitoring/#security-first","title":"Security First \ud83d\udd10","text":""},{"location":"security_monitoring/#aws-vpc-virtual-private-cloud","title":"\ud83d\udd12 AWS VPC (Virtual Private Cloud)","text":"<p>Our data systems run in a Virtual Private Cloud (VPC):</p> <ul> <li>An isolated, secure network within AWS  </li> <li>Prevents unauthorized external access  </li> <li>All services communicate inside this protected environment</li> </ul>"},{"location":"security_monitoring/#iam-identity-and-access-management","title":"\ud83d\udd12 IAM (Identity and Access Management)","text":"<p>IAM controls who can access what.</p> <ul> <li>Each team member is granted role-based access </li> <li>Least privilege principle is followed\u2014access is granted only as needed </li> <li>All actions are logged for audit and compliance</li> </ul>"},{"location":"security_monitoring/#kms-key-management-service","title":"\ud83d\udd12 KMS (Key Management Service)","text":"<p>We use AWS KMS to:</p> <ul> <li>Encrypt data at rest (stored in S3, Snowflake, etc.)  </li> <li>Encrypt data in transit (moving between services)  </li> <li>Manage encryption keys securely </li> </ul>"},{"location":"security_monitoring/#sql-injection-prevention","title":"\ud83d\udd12 SQL Injection Prevention","text":"<p>When interacting with databases (like Snowflake), our developers:</p> <ul> <li>Always use parameterized queries </li> <li>Avoid directly inserting user or file inputs into SQL statements  </li> <li>This prevents malicious attacks, known as SQL Injection</li> </ul>"},{"location":"security_monitoring/#security-best-practices-for-developers","title":"\ud83d\udd12 Security Best Practices for Developers","text":"Practice Why It Matters Never hardcode credentials Use environment variables or AWS Secrets Manager Follow least privilege principle Access only the services you need Use secure coding practices Prevent security vulnerabilities Encrypt sensitive data Protect PII, PHI, and client data Regularly review access policies Ensure no excessive permissions"},{"location":"security_monitoring/#always-watching-monitoring-everything","title":"Always Watching: Monitoring Everything \ud83d\udc40","text":"<p>We use multiple monitoring tools to ensure pipeline health, performance, and security.</p>"},{"location":"security_monitoring/#airflow-logs","title":"\ud83d\udcc4 Airflow Logs","text":"<ul> <li>View logs for every DAG run, task execution, and failure reason </li> <li>Logs are accessible via the Airflow UI </li> <li>Helps troubleshoot pipeline failures quickly  </li> </ul>"},{"location":"security_monitoring/#aws-cloudwatch","title":"\ud83d\udcca AWS CloudWatch","text":"<p>AWS CloudWatch monitors:</p> <ul> <li>Service performance (e.g., ECS tasks, S3 events)  </li> <li>Resource usage (CPU, memory, etc.)  </li> <li>System health (failures, bottlenecks, timeouts)</li> </ul> <p>CloudWatch is our first line of defense for identifying cloud infrastructure issues.</p> <p>(Internal Access Link - Requires VPN)</p>"},{"location":"security_monitoring/#snowflake-monitoring","title":"\ud83e\uddca Snowflake Monitoring","text":"<p>Snowflake provides its own monitoring features:</p> <ul> <li>Query profiling to analyze slow or costly queries  </li> <li>Warehouse utilization metrics to optimize performance  </li> <li>Storage and cost tracking for budget management  </li> </ul> <p>(Internal Access Link - Requires Snowflake Login)</p>"},{"location":"security_monitoring/#automated-alerts","title":"\ud83d\udea8 Automated Alerts","text":"<p>We have automated alerts set up for:</p> <ul> <li>Pipeline failures </li> <li>Unexpected data volumes </li> <li>Performance degradation </li> <li>Security anomalies</li> </ul> <p>When something goes wrong, the relevant team members are notified immediately.</p>"},{"location":"security_monitoring/#example-alert","title":"Example Alert:","text":"<pre><code>Subject: ETL Pipeline Failure\nBody: DAG 'sales\\_data\\_daily' failed during the Core Layer processing step.\nTime: 03:17 AM UTC\nAction Required: Check Airflow logs and resolve the issue.\n</code></pre>"},{"location":"security_monitoring/#summary","title":"Summary","text":"Area How We Handle It Network Security AWS VPC, IAM, KMS Database Security Parameterized queries, encryption Monitoring Airflow Logs, CloudWatch, Snowflake tools Alerts Automated notifications on failures <p>By combining strong security practices with comprehensive monitoring, we ensure that:</p> <ul> <li>Our data is protected </li> <li>Our pipelines run smoothly </li> <li>We can respond quickly to any issues</li> </ul>"},{"location":"pipeline/data_consumers/","title":"3.5. Data Consumers: Using the Data \ud83d\udcc8","text":"<p>Once data has been processed and validated, it becomes available to the end-users\u2014our Data Consumers.</p>"},{"location":"pipeline/data_consumers/#who-uses-the-data","title":"Who Uses the Data?","text":"<p>This is where you come in! Our processed data is consumed by various teams and tools across EVERSANA.</p>"},{"location":"pipeline/data_consumers/#examples-of-data-consumers","title":"Examples of Data Consumers:","text":"Role/Team How They Use the Data Data Analysts Build reports and dashboards using Tableau Data Scientists Train machine learning models, run analyses Business Users View reports, track KPIs, make data-driven decisions Downstream Applications Connect to Presentation Layer for real-time data"},{"location":"pipeline/data_consumers/#real-example-monthly-sales-dashboard","title":"Real Example: Monthly Sales Dashboard","text":"<ul> <li>Our Monthly Sales Dashboard in Tableau pulls data directly from the Presentation Layer in Snowflake.  </li> <li>This ensures the dashboard reflects up-to-date, cleansed, and validated data.</li> </ul>"},{"location":"pipeline/data_consumers/#how-do-consumers-access-the-data","title":"How Do Consumers Access the Data?","text":""},{"location":"pipeline/data_consumers/#primary-access-method","title":"Primary Access Method:","text":"<p>Most users access data via Snowflake Presentation Layer tables.</p> <p>These tables contain:</p> <ul> <li>Finalized, business-ready data </li> <li>Pre-aggregated or pre-joined views for specific use cases  </li> <li>Data that's ready for direct consumption with minimal additional processing</li> </ul>"},{"location":"pipeline/data_consumers/#access-tools","title":"Access Tools","text":"Tool Purpose Snowflake SQL-based data exploration Tableau Visual reporting and dashboards Jupyter/IDE Data science and ad hoc analysis APIs / Applications Real-time or batch data consumption"},{"location":"pipeline/data_consumers/#summary","title":"Summary","text":"<ul> <li>Presentation Layer = Consumer Layer </li> <li>It provides clean, structured, and ready-to-use data </li> <li>Consumers do not need to know the raw data structure\u2014the pipeline handles the complexity</li> </ul>"},{"location":"pipeline/data_processing/","title":"3.3. Orchestration: The Master Scheduler \u23f0","text":""},{"location":"pipeline/data_processing/#what-is-orchestration","title":"What is Orchestration?","text":"<p>Think of orchestration as the control tower of our data pipeline. It ensures that all data processing steps:</p> <ul> <li>Run on time </li> <li>Execute in the correct sequence </li> <li>Handle dependencies automatically</li> </ul> <p>Without orchestration, pipelines would be prone to:</p> <ul> <li>Missed schedules  </li> <li>Inconsistent runs  </li> <li>Manual intervention for every task</li> </ul>"},{"location":"pipeline/data_processing/#our-orchestration-tool-apache-airflow","title":"Our Orchestration Tool: Apache Airflow","text":"<p>We use Apache Airflow to orchestrate all data pipeline tasks.</p>"},{"location":"pipeline/data_processing/#key-features-of-airflow","title":"Key Features of Airflow:","text":"Feature Description DAGs Define workflows as Directed Acyclic Graphs Scheduling Run tasks on defined time intervals Sensors Wait for events, like new files in S3 Retry Logic Automatically retry failed tasks Monitoring &amp; Alerts Track DAG runs and receive notifications"},{"location":"pipeline/data_processing/#accessing-airflow","title":"Accessing Airflow","text":"<ul> <li>Web UI: Airflow provides a browser-based interface </li> <li>You can monitor DAGs, check logs, pause/resume tasks, and rerun failures  </li> </ul>"},{"location":"pipeline/data_processing/#how-it-works-dags-in-action","title":"How It Works: DAGs in Action","text":"<p>A DAG (Directed Acyclic Graph) represents the workflow.</p> <p>Each DAG:</p> <ul> <li>Defines what tasks to run </li> <li>Specifies the order of execution </li> <li>Determines when to run them</li> </ul>"},{"location":"pipeline/data_processing/#trigger-methods","title":"Trigger Methods:","text":"Trigger Type Example Scenario File Sensor Detects when a new file arrives in S3 Scheduled Run Runs at a set time, e.g., every day at 3 AM Manual Trigger Run manually from the Airflow UI for testing"},{"location":"pipeline/data_processing/#real-example","title":"Real Example:","text":"<p>DAG Name: <code>daily_sales_pipeline</code></p>"},{"location":"pipeline/data_processing/#workflow","title":"Workflow:","text":"<ol> <li>File Sensor waits for a new <code>sales_data</code> file in S3  </li> <li>Once detected, the DAG triggers data extraction  </li> <li>Followed by transformation and loading into Snowflake  </li> <li>Data Quality checks run automatically  </li> <li>Pipeline sends success/failure notifications</li> </ol>"},{"location":"pipeline/data_processing/#benefits-of-using-airflow","title":"Benefits of Using Airflow","text":"Benefit Why It Matters Automation No manual effort needed for pipeline runs Reliability Handles retries and failures gracefully Visibility Full view of pipeline status via UI Flexibility Can orchestrate across multiple systems"},{"location":"pipeline/data_processing/#summary","title":"Summary","text":"<ul> <li>Apache Airflow is our orchestration engine </li> <li>It controls when, how, and in what order our data pipelines run  </li> <li>It ensures reliability, automation, and visibility for our workflows</li> </ul>"},{"location":"pipeline/data_sources/","title":"3.1. Data Sources: Where Data Begins \ud83d\udce5","text":""},{"location":"pipeline/data_sources/#what-are-data-sources","title":"What Are Data Sources?","text":"<p>Data Sources are the starting point of our entire data pipeline. This is where all raw information originates before being processed.</p> <p>Without data sources, there is no data pipeline\u2014this is Step 1 in turning raw data into actionable insights.</p>"},{"location":"pipeline/data_sources/#examples-of-data-sources","title":"Examples of Data Sources","text":"<p>We collect data from multiple systems and external partners. Here are common examples:</p> Source Type Description &amp; Example CRM Systems Daily sales figures, customer records External Partners Third-party data feeds, vendor files APIs (Application Interfaces) Real-time data like survey responses or webhooks Operational Databases Internal transactional systems (e.g., orders, billing) Marketing Platforms Campaign performance data, clickstream events Public Data External datasets like market trends or weather"},{"location":"pipeline/data_sources/#how-it-works","title":"How It Works","text":""},{"location":"pipeline/data_sources/#file-types-formats","title":"File Types &amp; Formats","text":"<p>The raw data can arrive in various formats, including:</p> <ul> <li>CSV \u2013 Comma-Separated Values (most common for flat files)  </li> <li>Parquet \u2013 Optimized columnar storage for large datasets  </li> <li>JSON \u2013 Used for nested or API-based data  </li> <li>XML \u2013 Sometimes used by legacy systems  </li> <li>SQL Dumps \u2013 For large database extracts  </li> </ul>"},{"location":"pipeline/data_sources/#delivery-methods","title":"Delivery Methods","text":"Method Description S3 Files are uploaded directly on S3 SFTP / FTP Servers Files securely transferred to AWS S3 API Calls Data pushed to our systems via APIs Manual Uploads Occasionally, data is uploaded manually Streaming Events Real-time data streams (Kafka, Webhooks)"},{"location":"pipeline/data_sources/#data-landing-zone","title":"Data Landing Zone","text":"<p>All incoming data is first placed in our AWS S3 Landing Zone:</p> <ul> <li>A secure \"raw data\" storage location </li> <li>No transformations or changes are applied here  </li> <li>Serves as a data backup and audit point</li> </ul>"},{"location":"pipeline/data_sources/#why-this-matters","title":"Why This Matters","text":"<p>Understanding where the data comes from is critical because:</p> <ul> <li>It affects data quality </li> <li>Determines how the data should be processed </li> <li>Helps in troubleshooting issues when raw data is missing or incorrect</li> </ul>"},{"location":"pipeline/data_sources/#summary","title":"Summary","text":"<ul> <li>Data Sources are the entry point for the pipeline  </li> <li>Data arrives in various formats and from various systems </li> <li>Everything starts by landing raw data in AWS S3</li> </ul>"},{"location":"pipeline/ingestion_file_tracking/","title":"3.2. Ingestion &amp; File Tracking: Getting Data In \ud83d\udce6","text":""},{"location":"pipeline/ingestion_file_tracking/#what-is-ingestion","title":"What Is Ingestion?","text":"<p>Ingestion is the first technical step in our data pipeline after data is generated or sent by a source. It refers to the process of bringing raw data into our cloud environment so that it can be processed.</p>"},{"location":"pipeline/ingestion_file_tracking/#where-does-the-data-land","title":"Where Does the Data Land?","text":"<p>All incoming raw data is stored in AWS S3 Buckets.</p>"},{"location":"pipeline/ingestion_file_tracking/#what-is-aws-s3","title":"What is AWS S3?","text":"<ul> <li>Think of S3 as highly scalable, secure cloud folders </li> <li>It's a durable, cost-effective, and secure storage system </li> <li>We use it as our \"Landing Zone\" for raw data</li> </ul>"},{"location":"pipeline/ingestion_file_tracking/#example-file-arrival-in-s3","title":"Example: File Arrival in S3","text":"<p>Let\u2019s say the following file arrives:</p> <pre><code>sales\\_data\\_20250717.csv\n</code></pre> <p>When it lands in S3:</p> <ul> <li>The file is stored under a specific path like:</li> </ul> <p>```</p> <p>s3://eversana-raw-data/sales/2025/07/17/sales_data_20250717.csv</p> <p>````</p>"},{"location":"pipeline/ingestion_file_tracking/#how-do-we-track-incoming-files","title":"How Do We Track Incoming Files?","text":"<p>We use automated file tracking to record important information as files arrive.</p>"},{"location":"pipeline/ingestion_file_tracking/#what-we-track","title":"What We Track:","text":"Tracked Detail Description File Name Name of the incoming file S3 Path Exact cloud storage location File Size Size of the file in bytes Arrival Time When the file landed in the S3 bucket Source System Where the file originated from"},{"location":"pipeline/ingestion_file_tracking/#why-do-we-track-files","title":"Why Do We Track Files?","text":"<p>File tracking is critical because it:</p> <ul> <li>Provides end-to-end data observability </li> <li>Helps us verify data completeness (did we receive all expected files?)  </li> <li>Allows us to troubleshoot issues (e.g., missing files, duplicates, delayed arrivals)  </li> <li>Supports auditing and compliance requirements</li> </ul>"},{"location":"pipeline/ingestion_file_tracking/#where-is-this-information-stored","title":"Where Is This Information Stored?","text":"<p>We store file tracking details in special metadata tables in Snowflake, such as:</p>"},{"location":"pipeline/ingestion_file_tracking/#dynamic_ftb_input_universe-table","title":"DYNAMIC_FTB_INPUT_UNIVERSE Table","text":"Column Name What It Stores FILE ID Unique identifier for the file FILE NAME The actual file name (e.g., <code>sales_data_20250717.csv</code>) S3 FILE PATH Where the file is stored in S3 ARRIVAL TIMESTAMP Exact time the file arrived PRE VALIDATION STATUS Whether the file passed initial checks"},{"location":"pipeline/ingestion_file_tracking/#example-sql-query-snowflake","title":"Example SQL Query (Snowflake):","text":"<p>```sql SELECT \"FILE ID\", \"FILE NAME\", \"S3 FILE PATH\", \"ARRIVAL TIMESTAMP\" FROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_FTB_INPUT_UNIVERSE WHERE \"FILE NAME\" LIKE 'sales_data%' ORDER BY \"ARRIVAL TIMESTAMP\" DESC LIMIT 5; ````</p>"},{"location":"pipeline/ingestion_file_tracking/#summary","title":"Summary","text":"Step What Happens Data Arrival Raw files land in AWS S3 buckets File Tracking Begins File details are logged into Snowflake Audit Trail We maintain a full history of arrivals Next Step Pipeline reads the file for processing <p>By tracking every file that enters our system, we ensure pipeline reliability, data integrity, and full visibility into the data ingestion process.</p>"},{"location":"pipeline/orchestration/","title":"3.3. Orchestration: The Master Scheduler \u23f0","text":""},{"location":"pipeline/orchestration/#what-is-orchestration","title":"What is Orchestration?","text":"<p>Orchestration is like the control tower of our data pipeline.</p> <p>It ensures:</p> <ul> <li>Data processing steps run on time</li> <li>Tasks happen in the correct order</li> <li>Systems know when to wait and when to move forward</li> </ul> <p>Without orchestration, we'd have to manually kick off tasks or rely on basic cron jobs, which can be fragile and error-prone. Orchestration brings automation, reliability, and visibility to the process.</p>"},{"location":"pipeline/orchestration/#our-orchestration-tool-apache-airflow","title":"Our Orchestration Tool: Apache Airflow","text":"<p>At EVERSANA, we use Apache Airflow to manage and schedule all data pipeline workflows.</p>"},{"location":"pipeline/orchestration/#why-airflow","title":"Why Airflow?","text":"Feature What It Means DAGs Define workflows as Directed Acyclic Graphs Scheduling Run tasks on predefined intervals (e.g., daily at 3 AM) Event-based Triggers Start tasks when new files arrive (e.g., via S3 File Sensor) Retries &amp; Alerts Automatically handle failures and notify the team Visibility Web UI to monitor task status and logs"},{"location":"pipeline/orchestration/#accessing-airflow","title":"Accessing Airflow","text":"<p>Airflow provides a web-based interface (UI) where you can:</p> <ul> <li>Monitor DAGs </li> <li>Check logs of each task </li> <li>Pause / Resume workflows </li> <li>Rerun failed tasks</li> </ul> <p>\ud83d\udcc2 Access Airflow UI (Internal Link \u2013 Requires VPN / Internal Access Rights)</p>"},{"location":"pipeline/orchestration/#how-does-it-work","title":"How Does It Work?","text":""},{"location":"pipeline/orchestration/#what-is-a-dag","title":"What is a DAG?","text":"<p>A DAG (Directed Acyclic Graph) represents:</p> <ul> <li>A workflow or pipeline </li> <li>Defined by tasks (nodes) connected by dependencies (edges) </li> <li>No loops \u2013 data flows in one direction only</li> </ul>"},{"location":"pipeline/orchestration/#trigger-mechanisms","title":"Trigger Mechanisms:","text":"Trigger Type Example Scenario File Sensor Detect when a new file lands in S3 Scheduled Runs Run daily at a set time, e.g., 3:00 AM daily Manual Triggers Run manually from Airflow UI for testing/debugging"},{"location":"pipeline/orchestration/#real-example-daily_sales_pipeline","title":"Real Example: <code>daily_sales_pipeline</code>","text":"<p>Workflow:</p> <ol> <li>File Sensor: Waits for <code>sales_data</code> file in S3  </li> <li>Extraction: Pulls raw data from S3  </li> <li>Transformation: Cleans and processes the data  </li> <li>Load: Inserts the processed data into Snowflake  </li> <li>Data Quality Checks: Validates the load was successful  </li> <li>Notification: Sends a success/failure alert to the team</li> </ol>"},{"location":"pipeline/orchestration/#why-use-orchestration","title":"Why Use Orchestration?","text":"Benefit Why It Matters Automation No manual effort for pipeline runs Reliability Handles retries and error notifications Visibility Full monitoring via Airflow UI Scalability Easily add new workflows or adjust schedules Consistency Ensures data processes follow the same logic every time"},{"location":"pipeline/orchestration/#summary","title":"Summary","text":"<ul> <li>Apache Airflow is our Master Scheduler </li> <li>It controls when and how our pipelines run  </li> <li>Provides automation, visibility, and error handling </li> <li>Uses DAGs to define and manage workflows</li> </ul>"},{"location":"pipeline/overview/","title":"3. Our Data Pipeline: The Big Picture \ud83d\uddfa\ufe0f","text":""},{"location":"pipeline/overview/#overview","title":"Overview","text":"<p>Think of our data pipeline as a sophisticated factory.</p> <p></p> <p>Figure 1: High-level Architecture Diagram of EVERSANA's Data Pipeline</p> <ul> <li>Raw materials = Data </li> <li>Finished products = Business insights</li> </ul> <p>This factory doesn't just move data from point A to point B. It cleans, enriches, transforms, and delivers data so that it becomes useful for analytics, reporting, and decision-making.</p>"},{"location":"pipeline/overview/#why-does-it-matter","title":"Why Does It Matter?","text":"<p>At EVERSANA, data drives decisions. Our pipeline ensures that:</p> <ul> <li>Right data reaches the right people </li> <li>Data is accurate, secure, and on time </li> <li>We have full visibility into the data lifecycle</li> </ul>"},{"location":"pipeline/overview/#the-pipeline-stages","title":"The Pipeline Stages","text":"<p>Our pipeline is divided into several logical stages, each playing a unique role:</p>"},{"location":"pipeline/overview/#1-data-sources-where-data-begins","title":"1\ufe0f\u20e3 Data Sources: Where Data Begins \ud83d\udce5","text":"<ul> <li>This is where all raw data originates </li> <li>Could be from CRM systems, external vendors, APIs, or manual uploads</li> </ul> <p>Learn More \u2192 Data Sources</p>"},{"location":"pipeline/overview/#2-ingestion-file-tracking-getting-data-in","title":"2\ufe0f\u20e3 Ingestion &amp; File Tracking: Getting Data In \ud83d\udce6","text":"<ul> <li>Incoming data is stored in AWS S3 Buckets </li> <li>Every file is automatically tracked (file name, arrival time, size, etc.) in Snowflake metadata tables </li> <li>This allows monitoring and auditing</li> </ul> <p>Learn More \u2192 Ingestion &amp; File Tracking</p>"},{"location":"pipeline/overview/#3-orchestration-the-master-scheduler","title":"3\ufe0f\u20e3 Orchestration: The Master Scheduler \u23f0","text":"<ul> <li>We use Apache Airflow to control the sequence and timing of processing steps  </li> <li>Airflow uses DAGs (Directed Acyclic Graphs) to define workflows  </li> <li>Workflows can be triggered by time or when new data arrives</li> </ul> <p>Learn More \u2192 Orchestration</p>"},{"location":"pipeline/overview/#4-data-processing-turning-raw-into-ready","title":"4\ufe0f\u20e3 Data Processing: Turning Raw into Ready \u2699\ufe0f","text":"<ul> <li>Data is cleaned, transformed, enriched, and standardized </li> <li>We process data across different layers:</li> </ul> Layer Purpose Landing Layer Stores raw files exactly as received Raw Layer Initial formatting (e.g., column parsing) Core Layer Main transformations and enrichment Presentation Layer Final data products for reporting <p>Learn More \u2192 Data Processing</p>"},{"location":"pipeline/overview/#5-data-consumers-using-the-data","title":"5\ufe0f\u20e3 Data Consumers: Using the Data \ud83d\udcc8","text":"<ul> <li>Business analysts, data scientists, and applications use the processed data  </li> <li>Tools like Tableau and Power BI connect directly to the Presentation Layer in Snowflake </li> <li>Enables dashboards, models, and insights</li> </ul> <p>Learn More \u2192 Data Consumers</p>"}]}