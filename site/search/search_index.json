{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>EVERSANA ETL Process Runbook: Your Guide to Data at EVERSANA Version: 1.0 Date: July 17, 2025 Purpose: Welcome to the team! This document is your friendly introduction to how we handle data at EVERSANA. It will guide you through our core data pipeline, explaining how data moves from its source, gets processed, and becomes ready for use. No prior knowledge is needed \u2013 we'll explain everything step-by-step!</p> <ol> <li>Welcome to the Team! \ud83d\udc4b We're excited to have you on board! Data is at the heart of what we do at EVERSANA. This runbook will help you understand our data \"factory\" \u2013 how we bring in raw information, clean and organize it, and make it valuable for our business insights. Think of it as a map to our data landscape.</li> </ol> <p>Potential Improvements for this Section: Onboarding Checklist: Add a link to a general onboarding checklist for new hires, which could include initial setup steps for tools and systems.</p> <p>Team Directory: Include a link to a team directory or organizational chart to help new members understand who does what and who to reach out to.</p> <ol> <li>Quick Glossary: Understanding Our Lingo \ud83d\udcd6 Here are some common terms you'll encounter. Don't worry about memorizing them all at once; this section is here for quick reference! You can also find a full glossary here.</li> </ol> <p>ETL { data-tooltip=\"Extract, Transform, Load: Our main process! We Extract data from its source, Transform it (clean, reshape, combine), and then Load it into its final destination.\" }</p> <p>ELT { data-tooltip=\"Extract, Load, Transform: A variation where data is loaded into the target system first, then transformed within that system. We use both ETL and ELT concepts.\" }</p> <p>DAG { data-tooltip=\"Directed Acyclic Graph: In our Airflow system, this is like a flowchart that defines a series of tasks and the order they need to run in for a specific data process.\" }</p> <p>AWS { data-tooltip=\"Amazon Web Services: The 'cloud' where we host most of our data systems and services.\" }</p> <p>AWS S3 { data-tooltip=\"Simple Storage Service: Amazon's cloud storage. Imagine it as super-scalable, secure folders in the sky where we store our data files.\" }</p> <p>Apache Airflow { data-tooltip=\"An open-source tool that acts as our 'orchestrator' or 'scheduler.' It makes sure all our data processes run on time and in the correct order.\" }</p> <p>AWS ECS { data-tooltip=\"Elastic Container Service: An AWS service that helps us run our data processing applications in isolated 'containers.'\" }</p> <p>AWS Fargate { data-tooltip=\"A 'serverless' technology that works with ECS. It means we don't have to worry about managing the actual servers that run our applications; AWS handles it for us.\" }</p> <p>Docker { data-tooltip=\"A technology that packages our applications (and all their necessary parts) into self-contained units called 'containers.' This ensures our code runs exactly the same everywhere.\" }</p> <p>Snowflake { data-tooltip=\"Our powerful cloud-based data warehouse. This is where most of our processed data lives and where our analytical tools connect.\" }</p> <p>CI/CD { data-tooltip=\"Continuous Integration / Continuous Deployment: Our automated way of building, testing, and releasing new code changes quickly and reliably.\" }</p> <p>DQM { data-tooltip=\"Data Quality Monitoring: Our system for checking and ensuring that our data is accurate, complete, and consistent.\" }</p> <p>Metadata { data-tooltip=\"Data about data: Information stored in special tables that describes our files, columns, and the rules for processing them.\" }</p> <p>Potential Improvements for this Section: Interactive Glossary (Web Version): The data-tooltip attribute is used here for hover-over effects (requires Material for MkDocs theme and attr_list extension).</p> <p>Visual Aids: For complex terms, consider adding small icons or simple diagrams next to the definition.</p> <p>Pronunciation Guide: For acronyms or technical terms that might be hard to pronounce, a phonetic guide could be helpful.</p>"},{"location":"ci_cd/","title":"4. CI/CD Process","text":"<ol> <li>How We Build &amp; Deploy Code: Our CI/CD Process \ud83d\ude80 Our CI/CD { data-tooltip=\"Continuous Integration / Continuous Deployment: Our automated way of building, testing, and releasing new code changes quickly and reliably.\" } process is our automated pipeline for getting new code (for our data processes) from a developer's computer into our live systems. We use Azure DevOps { data-tooltip=\"A suite of development tools from Microsoft that we use for version control, CI/CD pipelines, and project management.\" } for this.</li> </ol> <p>Here's a simplified breakdown:</p> <p>You Write Code: As a developer, you'll write new code for our data processes (e.g., a new Apache Airflow DAG { data-tooltip=\"Directed Acyclic Graph: In our Airflow system, this is like a flowchart that defines a series of tasks and the order they need to run in for a specific data process.\" }, or a change to a data transformation script). You do this in a \"feature branch\" of our code repository (Azure Repos).</p> <p>Propose Changes (Pull Request - PR): When your code is ready, you create a \"Pull Request\" (PR). This is a request to merge your changes into the main codebase.</p> <p>Learn how to create a Pull Request in Azure DevOps</p> <p>Automated Checks (CI): Creating a PR automatically triggers our Continuous Integration (CI) pipeline. This pipeline:</p> <p>Checks Code Quality: It automatically scans your code for errors and best practices using tools like Sonar Cloud Analysis.</p> <p>Builds Components: It builds any necessary components, like new Docker images { data-tooltip=\"A technology that packages our applications (and all their necessary parts) into self-contained units called 'containers.' This ensures our code runs exactly the same everywhere.\" }.</p> <p>Important: Your PR can only be merged if these automated checks pass, and if other team members review and approve your code. Code reviews are crucial for maintaining quality!</p> <p>Merge to 'dev': Once approved, your changes are merged into our 'dev' (development) branch.</p> <p>Deploy to Development (CD): Merging to 'dev' automatically kicks off our Continuous Deployment (CD) pipeline for the development environment. This means your new code is automatically deployed to our development data pipeline, where we can test it thoroughly.</p> <p>Deploy to Production: After successful testing in the development environment and final approvals, your changes are merged into the 'main' (production) branch. This triggers another CD pipeline, deploying your changes to our live production environment.</p> <p>Potential Improvements for this Section: Actual CI/CD Diagram: Replace the placeholder (Imagine a simplified CI/CD flow diagram here) with the actual diagram from the original runbook.</p> <p>Screenshots: Include small screenshots of key steps in Azure DevOps (e.g., how to create a PR, what a passing pipeline looks like).</p> <p>\"Why CI/CD?\" Explanation: Briefly explain the benefits of CI/CD (e.g., faster deployments, fewer errors, more reliable code) for a new hire.</p> <p>Code Review Best Practices: Briefly mention the importance of code reviews as part of the PR process.</p>"},{"location":"deployement_guide/","title":"Deployement guide","text":"<ol> <li>How to Deploy Your Code: A Quick Guide (for Developers) \ud83d\ude80 If you're a developer, you'll be deploying your code changes. Here's a simplified version of the steps. Your team will provide more detailed, hands-on training.</li> </ol> <p>Get the Code: First, you'll \"clone\" our code repository (Azure Repos) to your computer using git clone.</p> <p>Common Git Command: git clone  <p>Work on 'dev': You'll usually start by making sure your local 'dev' branch is up-to-date: git checkout dev then git pull origin dev.</p> <p>Common Git Commands:</p> <p>git checkout : Switch to a branch. <p>git pull origin : Get latest changes from remote. <p>Make Your Changes: Open the relevant code file (e.g., an Apache Airflow DAG { data-tooltip=\"Directed Acyclic Graph: In our Airflow system, this is like a flowchart that defines a series of tasks and the order they need to run in for a specific data process.\" } file) and make your updates.</p> <p>Create a New Branch: Always work in a new \"feature branch\" for your changes: git checkout -b 'your-descriptive-branch-name'.</p> <p>Common Git Command: git checkout -b  <p>Save &amp; Commit: Save your changes, then \"add\" them to be tracked (git add .) and \"commit\" them with a clear message (git commit -m 'Your brief description').</p> <p>Common Git Commands:</p> <p>git add .: Stage all changes.</p> <p>git commit -m 'Your message': Commit staged changes.</p> <p>git status: Check current status of your working directory.</p> <p>git diff: See changes before staging/committing.</p> <p>Push Your Branch: Send your new branch to our central code repository: git push -u origin your-descriptive-branch-name.</p> <p>Common Git Command: git push -u origin  <p>Create a Pull Request (PR) to 'dev': Go to Azure DevOps (your team will provide the specific URL). You'll create a PR from your new branch to the 'dev' branch. This is where automated checks run and your teammates review your code.</p> <p>Merge to 'dev': Once your PR is approved and all checks pass, it gets merged into the 'dev' branch. This automatically deploys your changes to our development environment for testing.</p> <p>Create a PR to 'main' (for Production): After successful testing in the development environment, you'll create another PR, this time from the 'dev' branch to the 'main' branch. This also requires review and approval.</p> <p>Merge to 'main': Once approved, merging to 'main' automatically deploys your changes to our production environment (our live system).</p> <p>Verify: Always check the Airflow UI after deployment to make sure your changes are live and working as expected!</p> <p>Potential Improvements for this Section: Link to Detailed Guides: For each step, provide a link to a more detailed internal guide or a short video tutorial (if available).</p> <p>Common Git Commands: Add a small box with commonly used Git commands for developers (e.g., git status, git diff, git log).</p> <p>Troubleshooting Deployment Issues: Briefly mention common reasons for deployment failures and where to look for logs (e.g., \"If your pipeline fails, check the Azure DevOps pipeline logs\").</p>"},{"location":"glossary/","title":"Glossary","text":"<p>Full Glossary of Terms This page provides a comprehensive list of terms used throughout the runbook and within our data team.</p> <p>ETL (Extract, Transform, Load): Our main process! It means we Extract data from its source, Transform it (clean, reshape, combine), and then Load it into its final destination.</p> <p>Learn more about ETL</p> <p>ELT (Extract, Load, Transform): A variation where data is loaded into the target system first, then transformed within that system. We use both ETL and ELT concepts.</p> <p>Learn more about ELT</p> <p>DAG (Directed Acyclic Graph): In our Airflow system, this is like a flowchart that defines a series of tasks and the order they need to run in for a specific data process.</p> <p>Learn more about Airflow DAGs</p> <p>AWS (Amazon Web Services): The \"cloud\" where we host most of our data systems and services.</p> <p>Learn more about AWS</p> <p>AWS S3 (Simple Storage Service): Amazon's cloud storage. Imagine it as super-scalable, secure folders in the sky where we store our data files.</p> <p>Learn more about AWS S3</p> <p>Apache Airflow: An open-source tool that acts as our \"orchestrator\" or \"scheduler.\" It makes sure all our data processes run on time and in the correct order.</p> <p>Visit the Apache Airflow documentation</p> <p>AWS ECS (Elastic Container Service): An AWS service that helps us run our data processing applications in isolated \"containers.\"</p> <p>Learn more about AWS ECS</p> <p>AWS Fargate: A \"serverless\" technology that works with ECS. It means we don't have to worry about managing the actual servers that run our applications; AWS handles it for us.</p> <p>Learn more about AWS Fargate</p> <p>Docker: A technology that packages our applications (and all their necessary parts) into self-contained units called \"containers.\" This ensures our code runs exactly the same everywhere.</p> <p>Visit the Docker documentation</p> <p>Snowflake: Our powerful cloud-based data warehouse. This is where most of our processed data lives and where our analytical tools connect.</p> <p>Visit the Snowflake documentation</p> <p>CI/CD (Continuous Integration / Continuous Deployment): Our automated way of building, testing, and releasing new code changes quickly and reliably.</p> <p>Learn more about CI/CD</p> <p>DQM (Data Quality Monitoring): Our system for checking and ensuring that our data is accurate, complete, and consistent.</p> <p>Learn more about Data Quality</p> <p>Metadata: \"Data about data.\" In our world, this means information stored in special tables that describes our files, columns, and the rules for processing them.</p> <p>Learn more about Metadata</p> <p>Azure DevOps: A suite of development tools from Microsoft that we use for version control, CI/CD pipelines, and project management.</p> <p>Visit the Azure DevOps documentation</p> <p>Azure Repos: The Git-based code hosting service within Azure DevOps.</p> <p>Learn more about Azure Repos</p> <p>Tableau: A popular business intelligence tool used for data visualization and reporting.</p> <p>Visit the Tableau documentation</p> <p>Sonar Cloud Analysis: A cloud-based service for continuous code quality and security analysis.</p> <p>Visit SonarCloud documentation</p>"},{"location":"metadata_tables/","title":"6. Metadata Tables","text":"<ol> <li>Our \"Metadata\" Tables: The Pipeline's Brains \ud83e\udde0 Our data pipeline is smart because it's driven by metadata { data-tooltip=\"Data about data: Information stored in special tables that describes our files, columns, and the rules for processing them.\" }. This \"data about data\" tells our systems how to handle different files, what columns they have, and what transformations to apply. This metadata is stored in special tables within our Snowflake { data-tooltip=\"Our powerful cloud-based data warehouse. This is where most of our processed data lives and where our analytical tools connect.\" } data warehouse.</li> </ol> <p>These tables are crucial because they make our pipeline flexible. Instead of hardcoding rules into every script, we define them once in these tables, and our pipeline reads from them dynamically.</p> <p>Here are some of the most important metadata tables you'll hear about:</p> <p>DYNAMIC_DATA_LOADS:</p> <p>Purpose: This table holds all the instructions for how we load raw data.</p> <p>What it defines: It tells the pipeline things like the unique ID for a file (FILE ID), where the file is located in AWS S3 (S3 FILE PATH), how often we expect to receive it (FREQUENCY - daily, weekly), and the specific type of initial load (RAW LOAD_TYPE - e.g., \"FIRST_TIME\" for a new dataset, or \"TRUNCATE\" which means replacing all old data with new).</p> <p>Example Query:</p> <p>SELECT \"FILE ID\", \"S3 FILE PATH\", \"FREQUENCY\", \"RAW LOAD_TYPE\" FROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_DATA_LOADS LIMIT 5;</p> <p>DYNAMIC_COLUMN_DEFINITIONS:</p> <p>Purpose: This table is like a dictionary for our data columns. It defines the structure and data type for every column in our tables.</p> <p>What it defines: For each TABLE IDENTIFIER, it specifies the COLUMN NAME, what kind of data it should hold (DATATYPE - e.g., TEXT for words, FIXED for numbers, DATE for dates), and sometimes even custom rules for transforming the column's values (COLUMN_VALUE_TRANSFORMATION).</p> <p>Example Query:</p> <p>SELECT \"TABLE IDENTIFIER\", \"COLUMN NAME\", \"DATATYPE\", \"COLUMN_VALUE_TRANSFORMATION\" FROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_COLUMN_DEFINITIONS WHERE \"TABLE IDENTIFIER\" = 'YOUR_EXAMPLE_TABLE' LIMIT 5;</p> <p>DYNAMIC_CORE_LOADS:</p> <p>Purpose: This table contains the rules for how we process data in our Core Layer (where the main cleaning and transformation happens).</p> <p>What it defines: It specifies the type of load for the Core Layer (CORE LOAD TYPE - e.g., \"DELTA\" for incremental updates, \"UPSERT\" for merging new and existing data), and important flags like ENRICHMENT FLAG which tells the pipeline if additional \"enrichment\" steps are needed for the data.</p> <p>Example Query:</p> <p>SELECT \"TABLE IDENTIFIER\", \"CORE LOAD TYPE\", \"ENRICHMENT FLAG\" FROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_CORE_LOADS LIMIT 5;</p> <p>DYNAMIC_FTB_INPUT_UNIVERSE:</p> <p>Purpose: This table tracks the status of every incoming input file as it moves through each stage of the pipeline.</p> <p>What it defines: It records the file's ID, name, S3 path, and the status of various checks (e.g., \"pre-validation successful,\" \"raw load complete,\" \"core load failed,\" \"data quality monitoring run\"). This is crucial for troubleshooting if a file gets stuck or an error occurs.</p> <p>Example Query:</p> <p>SELECT \"FILE ID\", \"FILE NAME\", \"PRE VALIDATION\", \"CORE LOAD\", \"ERROR\" FROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_FTB_INPUT_UNIVERSE ORDER BY \"SYSTEM DATE\" DESC LIMIT 5;</p> <p>DYNAMIC_ENRICHMENT_LOADS:</p> <p>Purpose: This table stores special SQL queries that are run to \"enrich\" our data. Enrichment means adding more value or detail to existing data (e.g., looking up customer demographics based on an ID).</p> <p>What it defines: It links a specific file and table to a custom SQL_QUERY that will perform the enrichment operation.</p> <p>Example Query:</p> <p>SELECT \"TABLE_IDENTIFIER\", \"SQL_QUERY\" FROM EVERSANA_UTILITY_DB.MONITORING.DYNAMIC_ENRICHMENT_LOADS WHERE \"TABLE_IDENTIFIER\" = 'YOUR_ENRICHMENT_TABLE' LIMIT 1;</p> <p>Potential Improvements for this Section: Simple SQL Examples: For each metadata table, provide a very basic SQL SELECT statement that a new user could run in Snowflake to see what kind of data is in that table.</p> <p>Relationship Between Tables: Briefly explain how these tables relate to each other (e.g., \"The FILE ID in DYNAMIC_DATA_LOADS connects to the FILE ID in DYNAMIC_ENRICHMENT_LOADS\").</p> <p>\"Why Metadata?\" Explanation: Briefly explain why we use metadata tables (e.g., \"to make our pipeline flexible,\" \"to avoid hardcoding rules\").</p>"},{"location":"security_monitoring/","title":"5. Security & Monitoring","text":"<ol> <li>Staying Secure &amp; Monitoring Everything \ud83d\udd12\ud83d\udc40 Security and monitoring are built into every single step of our data pipeline.</li> </ol> <p>Security First: AWS VPC { data-tooltip=\"Virtual Private Cloud: Our data systems live in a secure, isolated network within AWS.\" } (Virtual Private Cloud): Our data systems live in a secure, isolated network within AWS.</p> <p>IAM { data-tooltip=\"Identity and Access Management: This controls who can access our AWS resources and what actions they are allowed to perform. Access is granted based on your role and what you need to do.\" } (Identity and Access Management): This controls who can access our AWS resources and what actions they are allowed to perform. Access is granted based on your role and what you need to do.</p> <p>KMS { data-tooltip=\"Key Management Service: We use this to encrypt our sensitive data, both when it's stored and when it's moving between systems.\" } (Key Management Service): We use this to encrypt our sensitive data, both when it's stored and when it's moving between systems.</p> <p>SQL Injection Prevention: When our code talks to databases (like Snowflake), we always use a secure method called \"parameterized queries.\" This is a technical detail, but it's crucial for preventing malicious attacks.</p> <p>Always Watching (Monitoring): Airflow Logs: We check Apache Airflow logs to see if our data workflows are running smoothly or if there are any errors.</p> <p>AWS CloudWatch: This is our main tool for monitoring the performance and health of all our AWS services (like AWS S3, AWS ECS tasks, etc.).</p> <p>Access AWS CloudWatch (Internal Link - Requires VPN/Access)</p> <p>Snowflake Monitoring: Snowflake has its own built-in tools that help us track how well our queries are performing and manage costs.</p> <p>Access Snowflake Monitoring (Internal Link - Requires Login)</p> <p>Alerts: We have automated alerts set up. If something goes wrong (like a pipeline failure, unexpected data volume, or performance issues), the right people are notified immediately.</p> <p>Potential Improvements for this Section: Accessing Monitoring Tools: Provide direct links or instructions on how a new user can access Airflow logs, AWS CloudWatch dashboards, and Snowflake monitoring interfaces.</p> <p>Example Alert: Show a very simple example of what an alert might look like (e.g., \"Email: ETL Pipeline Failure - DAG 'sales_data_daily' failed in Core Layer\").</p> <p>Security Best Practices for Developers: Briefly list common security practices relevant to developers (e.g., \"Never hardcode credentials,\" \"Follow least privilege principle\").</p>"},{"location":"pipeline/data_consumers/","title":"Data Consumers","text":"<p>3.5. Data Consumers: Using the Data \ud83d\udcc8 Who uses it: This is where you come in! Our processed data is used by various teams and tools.</p> <p>Examples: Data analysts use it for reports in tools like Tableau, data scientists use it for building models, and other applications connect to it. For instance, our monthly sales dashboard in Tableau directly pulls data from the Presentation Layer.</p> <p>How they access it: Most users access the data directly from the Presentation Layer tables in Snowflake.</p>"},{"location":"pipeline/data_processing/","title":"Data Processing","text":"<p>3.3. Orchestration: The Master Scheduler \u23f0 What it is: This stage is like our control tower, making sure all data processing steps run on time and in the correct sequence.</p> <p>Our Tool: We use Apache Airflow for this. Airflow provides a web-based UI where you can monitor DAGs.</p> <p>Access the Airflow UI (Internal Link - Requires VPN/Access)</p> <p>How it works: Airflow uses DAGs (those flowcharts we talked about!) to define our workflows. It can start a process when a new file is detected in S3 (via a \"file sensor\"), or it can run on a pre-defined schedule (e.g., every morning at 3 AM). For example, a DAG named daily_sales_pipeline might be configured to run once a new sales_data file lands in S3.</p>"},{"location":"pipeline/data_sources/","title":"Data Sources","text":"<p>3.1. Data Sources: Where Data Begins \ud83d\udce5 What it is: This is where all our raw information comes from.</p> <p>Examples: It could be data from our customer relationship management (CRM) systems, external partners, or other applications via APIs (Application Programming Interfaces). For instance, we might receive daily sales figures from a CRM system, or customer survey responses via an API.</p> <p>How it works: This raw data arrives in various formats, like CSV files or Parquet files.</p>"},{"location":"pipeline/ingestion_file_tracking/","title":"Ingestion & File Tracking","text":"<p>3.2. Ingestion &amp; File Tracking: Getting Data In \ud83d\udce6 What it is: The first step of bringing data into our cloud environment.</p> <p>Where it lands: All incoming raw data first lands in secure storage areas called AWS S3 buckets. Think of these as highly scalable, secure cloud folders.</p> <p>Tracking: As data arrives, we automatically record its details (like file name, size, and arrival time) in special \"file tracker\" tables. This helps us know exactly what data has arrived and when. For example, if a sales_data_20250717.csv file arrives, its name, timestamp, and S3 path are logged.</p>"},{"location":"pipeline/orchestration/","title":"Orchestration","text":"<p>3.3. Orchestration: The Master Scheduler \u23f0 What it is: This stage is like our control tower, making sure all data processing steps run on time and in the correct sequence.</p> <p>Our Tool: We use Apache Airflow for this. Airflow provides a web-based UI where you can monitor DAGs.</p> <p>Access the Airflow UI (Internal Link - Requires VPN/Access)</p> <p>How it works: Airflow uses DAGs (those flowcharts we talked about!) to define our workflows. It can start a process when a new file is detected in S3 (via a \"file sensor\"), or it can run on a pre-defined schedule (e.g., every morning at 3 AM). For example, a DAG named daily_sales_pipeline might be configured to run once a new sales_data file lands in S3.</p>"},{"location":"pipeline/overview/","title":"Overview","text":"<ol> <li>Our Data Pipeline: The Big Picture \ud83d\uddfa\ufe0f Think of our data pipeline as a sophisticated factory that turns raw materials (data) into finished products (insights). Here's a simplified view of how it works:</li> </ol> <p>Our pipeline is divided into several logical stages:</p> <p>Data Sources: Where our raw data originates.</p> <p>Ingestion &amp; File Tracker: How we bring data into our cloud and track its arrival.</p> <p>Orchestration: Our \"control tower\" that manages and schedules all processing steps.</p> <p>Data Processing: Where raw data gets cleaned, transformed, and prepared across different layers.</p> <p>Data Consumers: The people and tools that use the processed data.</p> <p>Click on the links in the navigation to explore each stage in detail!</p>"}]}